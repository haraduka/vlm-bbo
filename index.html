<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- TODO change title -->
    <title>
      Robotic Environmental State Recognition with Pre-Trained Vision-Language Models and Black-Box Optimization
    </title>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.7.0/font/bootstrap-icons.css" rel="stylesheet">
    <link rel="stylesheet" href="assets/css/main.css">

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-1XJ0NHR591"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-1XJ0NHR591');
    </script>

    <meta property="og:url"           content="https://haraduka.github.io/vlm-bbo" />
    <meta property="og:type"          content="website" />
    <meta property="og:title"         content="
      Robotic Environmental State Recognition with Pre-Trained Vision-Language Models and Black-Box Optimization
    " />
    <meta property="og:description"   content="
    In order for robots to autonomously navigate and operate in diverse environments, it is essential for them to recognize the state of their environment.
    On the other hand, the environmental state recognition has traditionally involved distinct methods tailored to each state to be recognized.
    In this study, we perform a unified environmental state recognition for robots through the spoken language with pre-trained large-scale vision-language models.
    We apply Visual Question Answering and Image-to-Text Retrieval, which are tasks of Vision-Language Models.
    We show that with our method, it is possible to recognize not only whether a room door is open/closed, but also whether a transparent door is open/closed and whether water is running in a sink, without training neural networks or manual programming.
    In addition, the recognition accuracy can be improved by selecting appropriate texts from the set of prepared texts based on black-box optimization.
    For each state recognition, only the text set and its weighting need to be changed, eliminating the need to prepare multiple different models and programs, and facilitating the management of source code and computer resource.
    We experimentally demonstrate the effectiveness of our method and apply it to the recognition behavior on a mobile robot, Fetch.
    " />
    <meta property="og:image" content="https://haraduka.github.io/vlm-bbo/assets/img/banner.png" />
  </head>
  <body>
    <div class="container-fluid">
      <div class="row">
        <div class="col-lg-8 offset-lg-2 col-md-12">

          <div class="text-center">
            <h1 class="mt-5"><b>Robotic Environmental State Recognition</b></h1>
    </title>
            <h2 class="mt-3"><b>with Pre-Trained Vision-Language Models and Black-Box Optimization</b></h1>
            <h4 class="mt-4 conf"><b>Advanced Robotics</b></h4>
            <ul class="list-inline mt-4">
              <li class="list-inline-item"><a href="https://haraduka.github.io" target="_blank">Kento Kawaharazuka</a></li>
              <li class="list-inline-item ml-4">Yoshiki Obinata</li>
              <li class="list-inline-item ml-4">Naoaki Kanazawa</li>
              <li class="list-inline-item ml-4">Kei Okada</li>
              <li class="list-inline-item ml-4">Masayuki Inaba</li>
              <li class="mt-2">
                JSK Robotics Laboratory, The University of Tokyo, Japan
              </li>
            </ul>
            <ul class="list-inline mt-4">
              <li class="list-inline-item">
                <a href="https://arxiv.org/abs/2405.04826" target="_blank">Paper</a>
              </li>
              <li class="list-inline-item ml-4">
                <a href="https://www.youtube.com/watch?v=aOoQcEdVb6M" target="_blank">Video</a>
              </li>
            </ul>
          </div>

          <div class="row mt-4">
            <div class="col-lg-10 offset-lg-1">
              <p>
    In order for robots to autonomously navigate and operate in diverse environments, it is essential for them to recognize the state of their environment.
    On the other hand, the environmental state recognition has traditionally involved distinct methods tailored to each state to be recognized.
    In this study, we perform a unified environmental state recognition for robots through the spoken language with pre-trained large-scale vision-language models.
    We apply Visual Question Answering and Image-to-Text Retrieval, which are tasks of Vision-Language Models.
    We show that with our method, it is possible to recognize not only whether a room door is open/closed, but also whether a transparent door is open/closed and whether water is running in a sink, without training neural networks or manual programming.
    In addition, the recognition accuracy can be improved by selecting appropriate texts from the set of prepared texts based on black-box optimization.
    For each state recognition, only the text set and its weighting need to be changed, eliminating the need to prepare multiple different models and programs, and facilitating the management of source code and computer resource.
    We experimentally demonstrate the effectiveness of our method and apply it to the recognition behavior on a mobile robot, Fetch.
              </p>
            </div>
          </div>

          <div class="col-lg-10 offset-lg-1"><hr class="mt-4 mb-4"></div>
          <div class="col-lg-10 offset-lg-1">
            <h4>Robotic Environmental State Recognition with Pre-Trained Vision-Language Models</h4>
            <p>
            The concept of this study: for the robotic environmental state recognition, we use pre-trained vision-language models BLIP2 and OFA for Visual Question Answering (VQA), and CLIP and ImageBind for Image-to-Text Retrieval (ITR), with black-box optimization to optimize the weighting of prepared text prompts.
            </p>
            <div align="center" class="row mt-4">
              <div class="col-lg-10 offset-lg-1">
                <img src="assets/img/concept.png" class="img-fluid">
              </div>
            </div>
          </div>

          <div class="col-lg-10 offset-lg-1"><hr class="mt-4 mb-4"></div>
          <div class="col-lg-10 offset-lg-1">
            <h4>Basic Experiments</h4>
            <p>
            The set of text prompts and representative images for Room, Elevator, Cabinet, Refrigerator, Microwave, Various Doors, Transparent Door, Light, Display, Handbag, Water, and Kitchen experiments.
            </p>
            <div align="center" class="row mt-4 mb-4">
              <div class="col-lg-10 offset-lg-1">
                <img src="assets/img/experiment.png" class="img-fluid">
              </div>
            </div>
            <p>
            The result of the state recognition experiment. The percentage of correct responses is shown for four different models.
            </p>
            <div align="center" class="row mt-4">
              <div class="col-lg-10 offset-lg-1">
                <img src="assets/img/table.png" class="img-fluid">
              </div>
            </div>
          </div>

          <div class="col-lg-10 offset-lg-1"><hr class="mt-4 mb-4"></div>
          <div class="col-lg-10 offset-lg-1">
            <h4>Advanced Experiment</h4>
            <p>
            Navigation experiment including recognition of the refrigerator door state, cabinet door state, and room door state.
            </p>
            <div align="center" class="row mt-4 mb-4">
              <div class="col-lg-10 offset-lg-1">
                <img src="assets/img/advanced.gif" class="img-fluid">
              </div>
            </div>
          </div>

          <!-- bibtex -->
          <div class="col-lg-10 offset-lg-1"><hr class="mt-4 mb-4"></div>
          <div class="col-lg-10 offset-lg-1">
            <h4 id="bibtex">Bibtex</h4>
            <pre>
@article{kawaharazuka2024vlmbbo,
  author={K. Kawaharazuka and Y. Obinata and N. Kanazawa and K. Okada and M. Inaba},
  title={{Robotic Environmental State Recognition with Pre-Trained Vision-Language Models and Black-Box Optimization}},
  journal={Advanced Robotics},
  pages={1--10},
  year=2024,
  doi={10.1080/01691864.2024.2366995},
}
            </pre>
          </div>

          <!-- contact -->
          <div class="col-lg-10 offset-lg-1"><hr class="mt-4 mb-4"></div>
          <div class="col-lg-10 offset-lg-1 mb-5">
            <h4 id="contact">Contact</h4>
            <p>
            If you have any questions, please feel free to contact
            <a href="https://haraduka.github.io" target="_blank">Kento Kawaharazuka</a>.
            </p>
          </div>

        </div>
      </div>
    </div>
  </body>
</html>
